{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and testing datasets are loaded into seperate numpy arrays.\n",
    "\n",
    "We are interested in generating a single digit, for example 5, so we extract all images containing 5 from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5421, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit5_images = []\n",
    "for i in range(len(train_images)):\n",
    "    if train_labels[i] == 5:\n",
    "        digit5_images.append(train_images[i])\n",
    "train_images = np.array(digit5_images)\n",
    "train_images.shape\n",
    "\n",
    "# The output shape is (5421, 28, 28) so we have 5421 images of size 28 x 28 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify we only have images contianing the digit 5\n",
    "\n",
    "n= 10\n",
    "f = plt.figure()\n",
    "for i in range(n):\n",
    "    f.add_subplot(1, n, i+1)\n",
    "    plt.subplot(1, n, i+1).axis('off')\n",
    "    plt.imshow(train_images[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we reshaped the data \n",
    "train_images = train_images.reshape(\n",
    "    train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "# As each colour in the images is in the range from 0 to 256, we normalise the values between -1 and 1.\n",
    "# The mean is 127.5 and the numbers are normalised as follows:\n",
    "\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "\n",
    "# We crate a batch dataset for training by calling the from_tensor_slices method\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(train_images.shape[0]).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Generator Model\n",
    "The generator's purpose is to create images containing the digit 5 which look similar to the images in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12544)             1254400   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 12544)            50176     \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 12544)             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 7, 7, 128)        819200    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 7, 7, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 14, 14, 64)       204800    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        1600      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,330,944\n",
      "Trainable params: 2,305,472\n",
      "Non-trainable params: 25,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating a Keras sequential model\n",
    "\n",
    "gen_model = tf.keras.Sequential()\n",
    "\n",
    "# Add a dense layer as the first layer\n",
    "gen_model.add(tf.keras.layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "# The input to this layer has been specified as 100 because later a noise vector of dimension 100 will\n",
    "# will be used as an input to this GAN model.\n",
    "\n",
    "# Next add a batch normalization layer to the model for providing stability.\n",
    "gen_model.add(tf.keras.layers.BatchNormalization())\n",
    "gen_model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "#  Reshape the output to 7x7x256\n",
    "gen_model.add(tf.keras.layers.Reshape((7, 7, 256)))\n",
    "\n",
    "# Use a Conv2D layer to upscale the generated image\n",
    "gen_model.add(tf.keras.layers.Conv2DTranspose(\n",
    "    128, (5, 5),\n",
    "    strides = (1,1),\n",
    "    padding = 'same',\n",
    "    use_bias = False))\n",
    "\n",
    "# Batch normalization and activation layers as before\n",
    "gen_model.add(tf.keras.layers.BatchNormalization())\n",
    "gen_model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "# Next a Conv2DTrasnpose layer with strides set to (2,2) is added followed by the batch normalization \n",
    "# and activation layers\n",
    "gen_model.add(tf.keras.layers.Conv2DTranspose(\n",
    "    64, (5 ,5),\n",
    "    strides = (2,2),\n",
    "    padding='same',\n",
    "    use_bias=False))\n",
    "\n",
    "gen_model.add(tf.keras.layers.BatchNormalization())\n",
    "gen_model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "# The final Conv 2D layer with strides queal to (2,2) is now added, further upscaling the image size to 28x28\n",
    "gen_model.add(tf.keras.layers.Conv2DTranspose(\n",
    "    1, (5,5),\n",
    "    strides=(2,2),\n",
    "    padding='same',\n",
    "    use_bias=False,\n",
    "    activation='tanh'))\n",
    "\n",
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x247f20ef880>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing the generator with a random input vector and display it\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = gen_model(noise, training=False)\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 28, 28, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the image\n",
    "generated_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 14, 14, 64)        1664      \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 6273      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 212,865\n",
      "Trainable params: 212,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "descri_model = tf.keras.Sequential()\n",
    "descri_model.add(tf.keras.layers.Conv2D(\n",
    "    64, (5,5),\n",
    "    strides=(2,2),\n",
    "    padding='same',\n",
    "    input_shape=[28, 28, 1]))\n",
    "\n",
    "descri_model.add(tf.keras.layers.LeakyReLU())\n",
    "descri_model.add(tf.keras.layers.Dropout(0.3))\n",
    "descri_model.add(tf.keras.layers.Conv2D(\n",
    "    128, (5,5),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "\n",
    "descri_model.add(tf.keras.layers.LeakyReLU())\n",
    "descri_model.add(tf.keras.layers.Dropout(0.3))\n",
    "descri_model.add(tf.keras.layers.Flatten())\n",
    "descri_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "# The descriminator uses just two convolutional layers. The output of the last convolutional layer\n",
    "# is of tyle (batch size, height, width, filters). The Flatten layer in the network flattens this\n",
    "# output to feed it to the last Dense layer in the network.\n",
    "\n",
    "descri_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00068314]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Testing the discriminator\n",
    "\n",
    "decision = descri_model(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "# Binary cross entropy used as we have two classes (1) for a real image and (0) for a fake one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator loss is defined as:\n",
    "def generator_loss(generated_output):\n",
    "    return cross_entropy(tf.ones_like(generated_output), generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The discriminator loss is defined as:\n",
    "def discriminator_loss(real_output, generated_output):\n",
    "    \n",
    "    # compute the loss considering the image  is real [1, 1, ..., 1]\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "\n",
    "    # compute loss considering the image is fake [0, 0, ...,0]\n",
    "    generated_loss = cross_entropy(tf.zeros_like(generated_output), generated_output)\n",
    "\n",
    "    # compute total loss\n",
    "\n",
    "    total_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the optimizers for both generator and discriminator\n",
    "gen_optimizer = tf.optimizers.Adam(1e-4)\n",
    "descri_optimizer = tf.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a few variables for training\n",
    "epoch_number = 0\n",
    "EPOCHS = 2\n",
    "noise_dim = 100\n",
    "seed = tf.random.normal([1, noise_dim])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint setup\n",
    "As the model training may take a long time intermediate states of the generator and discriminator are saved to a local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '/checkpoints/checkpoint'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir,'ckpt')\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer = gen_optimizer, discriminator_optimizer=descri_optimizer,\n",
    "generator=gen_model,\n",
    "discriminator=descri_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Setup\n",
    "Both the generator and discriminator will be trained in several steps. \n",
    "\n",
    "Gradient tape (tf.GradientTape) is used for automatic differentiation on both the generator and discriminator.\n",
    "\n",
    "At each step, a batch of images are given to the function as an input. The discriminator is asked to produce outputs for both the training and generated images. The training output is called as real and the generated image output as fake. The generator loss and discriminator loss is calculated on both the real and fake. Gradient tape is then used to compute the gradients on both losses and the apply the new gradients to the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_tuning(images):\n",
    "    # create a noise vector\n",
    "    noise = tf.random.normal([16, noise_dim])\n",
    "\n",
    "    #Use gradient tapes for automatic differentiation\n",
    "    with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n",
    "        \n",
    "        # ask generator to generate random images\n",
    "        generated_images = gen_model(noise, training=True)\n",
    "\n",
    "        # ask discriminator to evaluate real images and generate its output\n",
    "        real_output =   descri_model(images, training=True)\n",
    "\n",
    "        # ask discriminator to do the evaluatiion on generated (fake) images\n",
    "        fake_output = descri_model(generated_images, training=True)\n",
    "        \n",
    "        # calcualte generator loss on fake data\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "\n",
    "        # calculate discriminator loss as defined earlier\n",
    "        disc_loss = discriminator_loss(real_output,fake_output)\n",
    "\n",
    "    # calculate gradients for generator\n",
    "    gen_gradients = generator_tape.gradient(gen_loss, gen_model.trainable_variables)\n",
    "\n",
    "    # calculate gradients for discriminator\n",
    "    discri_gradients = discriminator_tape.gradient(disc_loss, descri_model.trainable_variables)\n",
    "\n",
    "    # use optimizer to process and apply gradients to variables\n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, gen_model.trainable_variables))\n",
    "\n",
    "    # same as above to discriminator\n",
    "    descri_optimizer.apply_gradients(zip(discri_gradients, descri_model)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "   \n",
    "    global epoch_number\n",
    "    epoch_number = epoch_number + 1\n",
    "   \n",
    "    # set training to false to ensure inference mode predictions\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    # display and save image\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:01d}.png'.format(epoch_number))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training function\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            gradient_tuning(image_batch)\n",
    "        \n",
    "        # produce images as we go\n",
    "        generate_and_save_images(gen_model, epoch +1, seed)\n",
    "\n",
    "        #save checkpoint data\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        print('Time for epoch {} is {} sec'.format(epoch+1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Sequential' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Matthew\\Dropbox\\T885-2\\GANs\\GAN\\Digit_GAN.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Matthew/Dropbox/T885-2/GANs/GAN/Digit_GAN.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# The model training is now started with a call to this train method\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Matthew/Dropbox/T885-2/GANs/GAN/Digit_GAN.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train(train_dataset, EPOCHS)\n",
      "\u001b[1;32mc:\\Users\\Matthew\\Dropbox\\T885-2\\GANs\\GAN\\Digit_GAN.ipynb Cell 30\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Matthew/Dropbox/T885-2/GANs/GAN/Digit_GAN.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Matthew/Dropbox/T885-2/GANs/GAN/Digit_GAN.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m image_batch \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Matthew/Dropbox/T885-2/GANs/GAN/Digit_GAN.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     gradient_tuning(image_batch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Matthew/Dropbox/T885-2/GANs/GAN/Digit_GAN.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# produce images as we go\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthew/Dropbox/T885-2/GANs/GAN/Digit_GAN.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m generate_and_save_images(gen_model, epoch \u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, seed)\n",
      "\u001b[1;32mc:\\Users\\Matthew\\Dropbox\\T885-2\\GANs\\GAN\\Digit_GAN.ipynb Cell 30\u001b[0m in \u001b[0;36mgradient_tuning\u001b[1;34m(images)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthew/Dropbox/T885-2/GANs/GAN/Digit_GAN.ipynb#X41sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m gen_optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gen_gradients, gen_model\u001b[39m.\u001b[39mtrainable_variables))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Matthew/Dropbox/T885-2/GANs/GAN/Digit_GAN.ipynb#X41sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# same as above to discriminator\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Matthew/Dropbox/T885-2/GANs/GAN/Digit_GAN.ipynb#X41sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m descri_optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39;49m(discri_gradients, descri_model))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Sequential' object is not iterable"
     ]
    }
   ],
   "source": [
    "# The model training is now started with a call to this train method\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4d981aab3083155095c2c074466ed07152749282d0ee32df655ab9e7e91ca8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
